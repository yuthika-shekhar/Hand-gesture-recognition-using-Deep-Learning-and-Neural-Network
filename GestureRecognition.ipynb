{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from skimage.transform import resize as imresize\n",
    "import imageio as imio\n",
    "import datetime\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default path written in the starter code threw error. We use the following cell to get the location , where we are running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-Processing\n",
    "\n",
    "Since the image size is not uniform across all the folders, let us figure out the available image sizes. We loop through each training folder and read the shape of the size of the first image in each of them.<br>\n",
    "We keep appending the sizes in a list called __image_size__ and print the result at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all folders in the train_path\n",
    "\n",
    "train_folder=os.listdir(train_path)\n",
    "\n",
    "# Create a blank list\n",
    "image_size=[]\n",
    "\n",
    "# Loop through each folder and read the 1st image.\n",
    "# Read in its shape and append it to the list image_size, if it\n",
    "# already doesn't exist.\n",
    "\n",
    "for folder in train_folder:\n",
    "    \n",
    "    file_path=train_path+'/'+folder\n",
    "    fst_image=os.listdir(train_path+'/'+folder)[0]\n",
    "    img=imio.imread(file_path+'/'+fst_image)\n",
    "    if img.shape not in image_size:\n",
    "        image_size.append(img.shape)\n",
    "\n",
    "print('The available Sizes Are')\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So , we've established that we've only two sizes. We will use __80x80__ as our reference to resize all images.<br> We initiate the important iteration parameters in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/mnt/disks/user/project/PROJECT/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/mnt/disks/user/project/PROJECT/Project_data/val.csv').readlines())\n",
    "\n",
    "#experiment with the batch size\n",
    "batch_size = 32\n",
    "ht=80\n",
    "wd=80\n",
    "num_epochs =60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Motion Detection\n",
    "\n",
    "After numerous futile methods , we tried to figure out what was common to all our models. We realized that the images are of poor quality and were taken at a typical workplace. That's why the images were plagued with many bright spots.<br> We then decided to explore feature extraction. In all the videos, the individual images weren't important. Rather the difference between each consecutive image was the key feature.<br> Thus, we decided to feed the difference between consecutive images into the model-fit methos. <br> We demonstrate an example below in the following two cells to do the following<br>\n",
    "\n",
    "- Select a folder at random from __train_doc__\n",
    "- Read in all the image file names into a list called __images__.\n",
    "- Iterate through the 30 images and calculate the difference between each image.\n",
    "- Invert the difference and plot.\n",
    "- In the end , we will have 29 difference images.\n",
    "- In some difference plots, we see all blank. This means there was no movement between two consecutive images.\n",
    "- There are 29 differences, so last subplot is empty.\n",
    "\n",
    "__This is the key EDA we did to eliminate all irrelevant information from the training data. We will feed only the 29 difference images into the model for training__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.permutation(train_doc)\n",
    "path='/mnt/disks/user/project/PROJECT/Project_data/train/'+t[0].split(';')[0]\n",
    "images=os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(5, 6, sharey=True,sharex=True)\n",
    "f.set_figwidth(15)\n",
    "f.set_figheight(15)\n",
    "\n",
    "images.sort()\n",
    "print(t[0].split(';')[1])\n",
    "for r in range(0,5):\n",
    "    for c in range(0,6):\n",
    "        i=c+r*6\n",
    "        if i<29:\n",
    "            \n",
    "            # Read the ith and i+1th image and resize them to 80x80\n",
    "            path1=path+'/'+images[i]\n",
    "            path2=path+'/'+images[i+1]\n",
    "            \n",
    "            img1=cv2.imread(path1).astype(np.float32)\n",
    "            img1=imresize(img1,(80,80))\n",
    "            \n",
    "            img2=cv2.imread(path2).astype(np.float32)\n",
    "            img2=imresize(img2,(80,80))\n",
    "            \n",
    "            # Calculate the absolute difference between two consecutive images\n",
    "            img3=cv2.absdiff(img1,img2)\n",
    "            imagem = abs(img3 - 255)\n",
    "            ax[r,c].imshow(imagem/255)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generator\n",
    "\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.<br>\n",
    "\n",
    "The generator code functioning is described below:\n",
    "\n",
    "- We've 663 folders in training dataset, with a batch-size of 32.\n",
    "- This means, in one run, we'll have 663/32=20 ( integer div ) folders in each batch.\n",
    "- After 32 batches, we would cover 32x20=640 folders and 23 folders would still remain.\n",
    "- We address this issue by inserting these remaining folders into the last batch of a single run.\n",
    "- The first __for loop__ has __batch__ iterating from 0 to 32 , as our __num_bathes=32__\n",
    "- The second __for loop__ has __folder__ iterating from 0 to 19.\n",
    "- When we read in the images from a folder, we calculate the difference between each consecutive image and feed it into the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    \n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    # create a list of image numbers you want to use for a particular video\n",
    "    # Instead of feeding every image into the training process, we produce 29 differences.\n",
    "    \n",
    "    img_idx = range(0,29)\n",
    "    \n",
    "    # The number of images per sequence is determined by the length of img_idx\n",
    "    \n",
    "    x=len(img_idx)\n",
    "    \n",
    "    # We'll use these dimensions for our resizing the images , as established above.\n",
    "    ht=80\n",
    "    wd=80\n",
    "   \n",
    "    while True:\n",
    "        \n",
    "        # t is the folder list but fully randomised\n",
    "        t = np.random.permutation(folder_list)\n",
    "        \n",
    "        # calculate the number of batches\n",
    "        num_batches =len(t)//batch_size\n",
    "        \n",
    "        # we iterate over the number of batches from 0 to 19 ( for batch_size = 32 )\n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            batch_data = np.zeros((batch_size,x,ht,wd,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            # folder iterates from 0 to 31 for a batch size of 32.\n",
    "            for folder in range(batch_size):\n",
    "                \n",
    "                # i is an index to iterate through the randomised folder list t\n",
    "                i=folder+(batch*batch_size)\n",
    "                \n",
    "                # list all images in the folder identified by the index i\n",
    "                imgs = os.listdir(source_path+'/'+ t[i].split(';')[0])\n",
    "                \n",
    "                # The image file list in imgs above aren't necessarily sorted. There is a danger of\n",
    "                # losing the temporal information. That's why we sort the list below.\n",
    "                \n",
    "                imgs.sort()\n",
    "                \n",
    "                # Iterate iver the frames/images of a folder to read them in\n",
    "                \n",
    "                for idx,item in enumerate(img_idx): \n",
    "                    img1 = cv2.imread(source_path+'/'+ t[i].strip().split(';')[0]+'/'+imgs[idx]).astype(np.float32)\n",
    "                    img2 = cv2.imread(source_path+'/'+ t[i].strip().split(';')[0]+'/'+imgs[idx+1]).astype(np.float32)\n",
    "                    \n",
    "                    # The key idea is to capture motion from the images. Instead of reading 30 images,\n",
    "                    # we calculate the difference between consecutive images and feed it into the model.\n",
    "                    \n",
    "                    diff_image=cv2.absdiff(img1,img2)\n",
    "                    image = abs(diff_image - 255)/255\n",
    "                    \n",
    "                    image = imresize(image, (ht, wd))\n",
    "                     \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:, :, 0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:, :, 1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:, :, 2]\n",
    "                    \n",
    "            batch_labels[folder, int(t[i].strip().split(';')[2])] = 1    \n",
    "            yield batch_data,batch_labels\n",
    "        \n",
    "        # We now make a batch of the remaining folders.\n",
    "        # the variable i has the last folder index we squeezed into the batch.\n",
    "        # Incrementing it by 1 , will point to the next folder for the new batch.\n",
    "        \n",
    "        rem_folders=len(t)-(i+1)\n",
    "        batch_labels = np.zeros((rem_folders,5))\n",
    "        batch_data = np.zeros((rem_folders,x,ht,wd,3))\n",
    "        \n",
    "        # m is counter for iterating through the remaining folders  \n",
    "        for m in range(0,rem_folders):\n",
    "            \n",
    "            # m+i+1 is now an index pointing to folders in t, that were left above.\n",
    "            imgs = os.listdir(source_path+'/'+ t[m+i+1].split(';')[0]) # read all the images in the folder\n",
    "            imgs.sort()\n",
    "            for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder to read them in\n",
    "                \n",
    "                img1 = cv2.imread(source_path+'/'+ t[m+i+1].strip().split(';')[0]+'/'+imgs[idx]).astype(np.float32)\n",
    "                img2 = cv2.imread(source_path+'/'+ t[m+i+1].strip().split(';')[0]+'/'+imgs[idx+1]).astype(np.float32)\n",
    "                \n",
    "                diff_image=cv2.absdiff(img1,img2)\n",
    "                \n",
    "                # We normalize here instead of the individual channels \n",
    "                image = abs(diff_image - 255)/255\n",
    "                \n",
    "                image = imresize(image, (ht, wd))\n",
    "                batch_data[m,idx,:,:,0] = image[:, :, 0]\n",
    "                batch_data[m,idx,:,:,1] = image[:, :, 1]\n",
    "                batch_data[m,idx,:,:,2] = image[:, :, 2]\n",
    "            \n",
    "            batch_labels[m, int(t[m+i+1].strip().split(';')[2])] = 1\n",
    "        \n",
    "        yield batch_data,batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "# choose the number of epochs\n",
    "\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Testing the Generator Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the generator code by using a compact training set of 15 folders and a batch size of 6.<br>\n",
    "This means , the generator code should give us three batches of 6,6 and 3 folders each in one run. We simulate this reading in only the 1st 15 folders into __small_train_doc__ and calling the genrator thrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_doc = train_doc[0:15]\n",
    "x=generator(train_path, small_train_doc, 6)\n",
    "\n",
    "batch_data,batch_labels=next(x)[:]\n",
    "print(batch_data.shape,batch_labels.shape)\n",
    "batch_data,batch_labels=next(x)[:]\n",
    "print(batch_data.shape,batch_labels.shape)\n",
    "batch_data,batch_labels=next(x)[:]\n",
    "print(batch_data.shape,batch_labels.shape)\n",
    "print('Success !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout,LSTM\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv3D,MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam.\n",
    "\n",
    "In the model building section, we build 3 models.\n",
    "\n",
    "- Conv2D+GRU Model Runnning on pre-processed images with Adam Optmizer\n",
    "- Conv3D Model Runnning on pre-processed images with Adam Optmizer\n",
    "- Conv3D Model Running __without__ pre-processed images with SGD Optmizer __( This is our final Submission )__\n",
    "\n",
    "<span style=\"color:red\"> __Final Submitted Model is from Section 3.3 Conv3D. Kindly jump to Section 3.3__ </span>\n",
    "\n",
    "### 3.1 Three Conv2D Layers Followed by One GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(TimeDistributed(Conv2D(32,(3,3),padding='same',activation='relu'), input_shape=(29, ht, wd, 3)))\n",
    "model.add(TimeDistributed(Conv2D(32,(3,3),activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
    "\n",
    "# Layer 2\n",
    "model.add(TimeDistributed(Conv2D(64,(3,3),padding='same',activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
    "\n",
    "# Layer 3\n",
    "model.add(TimeDistributed(Conv2D(128,(3,3),padding='same',activation='relu')))\n",
    "#model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# Entering GRU RNN\n",
    "model.add(GRU(256, activation='relu', return_sequences=False))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list,validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us call the __history__ output to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(5)\n",
    "\n",
    "fig.suptitle('Model 1 Performance')\n",
    "ax1.plot(history.history['loss'],label='Training')\n",
    "ax1.plot(history.history['val_loss'],label='Validation')\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_ylim(0, 0.2)\n",
    "ax1.legend(loc=\"upper right\")\n",
    "\n",
    "ax2.plot(history.history['categorical_accuracy'],label='Training')\n",
    "ax2.plot(history.history['val_categorical_accuracy'],label='Validation')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_ylim(0.0, 1)\n",
    "ax2.legend(loc=\"bottom right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({ key: pd.Series(val) for key, val in history.history.items()}).to_csv('Model with Pre-processing 28th Feb 60 Epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Conv2D 60 Epochs 29th Feb.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Conv2D+GRU Model seems to be converging well after Epoch 40, but cost remains surprisingly steady !!\n",
    "\n",
    "We will now attempt a Conv3D model. Since the pre-processed images were very successful in the CNN+GRU Model, we'll continue to use them.<br>\n",
    "We'll make a minor change here in the batch size and make it 15 instead of 32 to deal with __kernel died__ problem. The model layers remain exactly same. <br>\n",
    "However, instead of __Time Distributed__ we use __Conv3D__ here. This implies changing the Kernels to 3D as well. \n",
    "\n",
    "### 3.2 Conv3D on Pre-Processed Images\n",
    "\n",
    "This model will take the difference images as the Conv2D Model. Otherwise all other steps remain same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model2.add(Conv3D(32, (3,3,3), activation='relu', input_shape=(29, ht, wd, 3)))\n",
    "model2.add(Conv3D(32, (3,3,3), activation='relu'))\n",
    "model2.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model2.add(Conv3D(64, (3,3,3), activation='relu'))\n",
    "model2.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "model2.add(Conv3D(128, (3,3,3), activation='relu'))\n",
    "model2.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    " \n",
    "# Layer 4\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(256))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001)\n",
    "model2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model remains shallow, but the parameter count has reached 4 Million. In the cell below, we change the batch size and initialize the generators again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size2=15\n",
    "train_generator = generator(train_path, train_doc, batch_size2)\n",
    "val_generator = generator(val_path, val_doc, batch_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init_3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size2) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size2)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size2) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size2)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint2 = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                              save_weights_only=False, mode='auto', period=1)\n",
    "callbacks_list2 = [checkpoint2]\n",
    "history2=model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list2,validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig3.set_figwidth(20)\n",
    "fig3.set_figheight(5)\n",
    "\n",
    "fig3.suptitle('Model 2 Conv 3D Performance')\n",
    "ax1.plot(history2.history['loss'],label='Training')\n",
    "ax1.plot(history2.history['val_loss'],label='Validation')\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_ylim(0, 3)\n",
    "ax1.legend(loc=\"upper right\")\n",
    "\n",
    "ax2.plot(history2.history['categorical_accuracy'],label='Training')\n",
    "ax2.plot(history2.history['val_categorical_accuracy'],label='Validation')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_ylim(0.4, 1)\n",
    "ax2.legend(loc=\"bottom right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({ key: pd.Series(val) for key, val in history2.history.items()}).to_csv('Model2 Shallow with Pre-processing 28th Feb Conv3D.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"Conv3D 60 Epochs.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Although the peformance of the __Time Distributed Conv2D__ and the __Conv3D__ models were very good on the pre-processed images, we cannot use them for submission. This is because, we do not have access to test data. Without having the test data-set, we are not sure , how to apply pr-processing on test data.<br></span>\n",
    "\n",
    "<span style=\"color:red\">Therefore , we build the final model below __Conv3D Without__ Pre-processing.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Conv3D Without Pre-processing\n",
    "\n",
    "This is the final Conv3D version. Here, we will use the images as they are. However, the following changes will be implemented.\n",
    "\n",
    "- The Generator function at the top was subtracting Nth image from N+1th image and inserting 29 difference images into the batch. __We will not do that here__.\n",
    "- In this case, we will keep the images as they are. However, we'll take every altertae image from each frame. __Our Image count will be thus 15 per frame__. Thus we define another __generator2__.\n",
    "- We will increase the epochs to __80__ giving the model more time to converge.\n",
    "- Our layer structure becomes deeper than the previous variant but parameter count is lesses.\n",
    "- Input shape now becomes __(15,80,80,3)__ instead of __(29,80,80,3)__\n",
    "- Instead of Adam, we use SGD with Nesterov Momentum for optimizer. We also implement __Reduce Learning Rate on Plateau__ in callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator2(source_path, folder_list, batch_size):\n",
    "    \n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = range(0,30,2)\n",
    "    x=len(img_idx)\n",
    "    ht=80\n",
    "    wd=80\n",
    "    while True:\n",
    "        \n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches =len(t)//batch_size\n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            batch_data = np.zeros((batch_size,x,ht,wd,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            \n",
    "            for folder in range(batch_size):\n",
    "                i=folder+(batch*batch_size)\n",
    "                imgs = os.listdir(source_path+'/'+ t[i].split(';')[0])\n",
    "                imgs.sort()\n",
    "                for idx,item in enumerate(img_idx): \n",
    "                    img1 = cv2.imread(source_path+'/'+ t[i].strip().split(';')[0]+'/'+imgs[idx]).astype(np.float32)\n",
    "                    image = imresize(img1, (ht, wd))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:, :, 0]/255\n",
    "                    batch_data[folder,idx,:,:,1] = image[:, :, 1]/255\n",
    "                    batch_data[folder,idx,:,:,2] = image[:, :, 2]/255\n",
    "                    \n",
    "            batch_labels[folder, int(t[i].strip().split(';')[2])] = 1    \n",
    "            yield batch_data,batch_labels\n",
    "        \n",
    "        rem_folders=len(t)-(i+1)\n",
    "        batch_labels = np.zeros((rem_folders,5))\n",
    "        batch_data = np.zeros((rem_folders,x,ht,wd,3))\n",
    "        for m in range(0,rem_folders):\n",
    "            \n",
    "            imgs = os.listdir(source_path+'/'+ t[m+i+1].split(';')[0])\n",
    "            imgs.sort()\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                \n",
    "                img1 = cv2.imread(source_path+'/'+ t[m+i+1].strip().split(';')[0]+'/'+imgs[idx]).astype(np.float32)\n",
    "                image = imresize(img1, (ht, wd))\n",
    "                batch_data[m,idx,:,:,0] = image[:, :, 0]/255\n",
    "                batch_data[m,idx,:,:,1] = image[:, :, 1]/255\n",
    "                batch_data[m,idx,:,:,2] = image[:, :, 2]/255\n",
    "            \n",
    "            batch_labels[m, int(t[m+i+1].strip().split(';')[2])] = 1\n",
    "        \n",
    "        yield batch_data,batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model3.add(Conv3D(32, (3,3,3), activation='relu', input_shape=(15, 80, 80, 3)))\n",
    "model3.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model3.add(Conv3D(64, (3,3,3), activation='relu'))\n",
    "model3.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "model3.add(Conv3D(128, (3,3,3), activation='relu'))\n",
    "model3.add(Conv3D(128, (3,3,3), activation='relu'))\n",
    "model3.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "model3.add(Conv3D(256, (2,2,2), activation='relu'))\n",
    "model3.add(Conv3D(256, (2,2,2), activation='relu'))\n",
    "model3.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "# Layer 5\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(1024))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(1024))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimiser = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size2=15\n",
    "num_epochs2 =80\n",
    "train_generator = generator2(train_path, train_doc, batch_size2)\n",
    "val_generator = generator2(val_path, val_doc, batch_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init_3D_no_pre_process' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size2) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size2)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size2) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size2)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size2) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous two models, we used Adam optimiser, so we did not implement Reduce LR on Plateau. Here we switch to an SGD Optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint3 = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False,\n",
    "                              save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "LR =ReduceLROnPlateau(monitor='loss',factor=0.1,patience=2,verbose=0,mode='min',epsilon=0.001,cooldown=0,min_lr=0)\n",
    "callbacks_list2 = [checkpoint3,LR]\n",
    "\n",
    "history3=model3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs2, verbose=1, \n",
    "                    callbacks=callbacks_list2,validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig4.set_figwidth(20)\n",
    "fig4.set_figheight(5)\n",
    "\n",
    "fig4.suptitle('Model 3 Conv 3D Performance No Pre Processing')\n",
    "ax1.plot(history3.history['loss'],label='Training')\n",
    "ax1.plot(history3.history['val_loss'],label='Validation')\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_ylim(0, 0.5)\n",
    "ax1.legend(loc=\"upper right\")\n",
    "\n",
    "ax2.plot(history3.history['categorical_accuracy'],label='Training')\n",
    "ax2.plot(history3.history['val_categorical_accuracy'],label='Validation')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_ylim(0.0, 1)\n",
    "ax2.legend(loc=\"bottom right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that , Epochs between __10 to 80__ are well fitting the validation data. We will pick our submission model from one of these Epochs.<br>\n",
    "Once again, our loss remains static. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
